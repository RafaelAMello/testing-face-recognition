{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker_hostname = 'kafka-49b7861-rafaelathaydemello-3b01.aivencloud.com'\n",
    "kafka_broker_port = '25697'\n",
    "kafka_broker = kafka_broker_hostname + ':' +kafka_broker_port\n",
    "kafka_topic= 'picture-metadata'\n",
    "\n",
    "spark = SparkSession.builder.appName('PicturesAnalysis').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# Batch Query\n",
    "df = spark.read.format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', kafka_broker)\\\n",
    "    .option(\"kafka.security.protocol\",\"SSL\")\\\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"\")\\\n",
    "    .option(\"kafka.ssl.truststore.location\", \"/home/jovyan/local/client.truststore.jks\")\\\n",
    "    .option(\"kafka.ssl.truststore.password\", \"password\")\\\n",
    "    .option(\"kafka.ssl.keystore.type\", \"PKCS12\")\\\n",
    "    .option('kafka.ssl.keystore.location', \"/home/jovyan/local/client.keystore.p12\")\\\n",
    "    .option('kafka.ssl.keystore.password', \"password\")\\\n",
    "    .option('subscribe', kafka_topic)\\\n",
    "    .option('startingOffsets', 'earliest')\\\n",
    "    .option(\"includeHeaders\", \"true\")\\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .option(\"sslmode\", \"require\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+---------+------+--------------------+-------------+-------+\n",
      "|                 key|               value|           topic|partition|offset|           timestamp|timestampType|headers|\n",
      "+--------------------+--------------------+----------------+---------+------+--------------------+-------------+-------+\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     0|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     1|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     2|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     3|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     4|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     5|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     6|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     7|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     8|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|     9|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    10|2020-08-09 04:04:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    11|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    12|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    13|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    14|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    15|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    16|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    17|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    18|2020-08-09 04:08:...|            0|   null|\n",
      "|[32 30 32 30 2D 3...|[7B 22 73 75 63 6...|picture-metadata|        0|    19|2020-08-09 04:08:...|            0|   null|\n",
      "+--------------------+--------------------+----------------+---------+------+--------------------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Query\n",
    "df = spark.readStream.format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', kafka_broker)\\\n",
    "    .option('sasl.cafile', \"ca.pem\")\\\n",
    "    .option('ssl.certfile', 'service.cert')\\\n",
    "    .option('ssl.keyfile', 'service.key')\\\n",
    "    .option('subscribe', kafka_topic)\\\n",
    "    .option(\"includeHeaders\", \"true\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();;\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1a6ce2362cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();;\nkafka"
     ]
    }
   ],
   "source": [
    "df.writeStream.format(\"memory\").queryName(\"hello\").start()\n",
    "raw = spark.sql(\"select * from hello\")\n",
    "raw.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
